{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EVENT CONSUMER CLIENT - Apache Spark Streaming<hr />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The consumer client is implemented by Apache Spark Streaming client which listens to the topic where the data is being ingested. The client accepts streams of data from all the producers, performs stream level operations in accordance to several operational conditions and eventually ingested into respective collections in the MongoDb database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the implementation of the Spark streaming engine, we have made some assumptions to ensure that the streaming, processing and ingestion into the database happens in a seamless manner. Following are the assumptions:\n",
    "\n",
    "<ol>\n",
    "    <li>The streaming application will ingest the processed data to the collections created to contain the histori data.</li>\n",
    "    <li>The streaming application will be accepting streams of data from a single stream as a batch/partition of RDD's. The engine will go through each of the partition, checks for conditions mentioned in the business logic and takes actions; Operational or ingestional.</li>\n",
    "    <li>For each stream of the data that is accepted, for each chunk of data within the stream recieved from each of the producer, we will be appending <b>geohash</b> key for each partition. The geohash is implemented by making use of third-party library using the latitude and longitude information present in each of the chunks of the streamed data. </li>\n",
    "    <li>For comparing any two geohashes for 2 data chunks, we are considering the first 3 prefixes of the gephashes to be same for comparison. if the first 3 prefixes of any two chunks from any 2 producers is the same, we assume these 2 chunks belonging to the same region considering geohash represents a rectangular region on the map and plots the data points on the geogrpahical map</li>\n",
    "    <li>We come across 3 conditions while processing the streamed data for the geohashes having similar prefixes:</li>\n",
    "    <ul>\n",
    "        <li>Two partitions : Hotspot-AQUA and Hotspot-TERRA; we perform the average of surface_temperature and confidence and ingest the transformed data to the database</li>\n",
    "        <li>If the partitions are from Climate and Hotspot(AQUA OR TERRA), we append the objectID of the Hotspot to the Climate partition and ingest the modified partitions to Climate and Hotspot collections created.</li>\n",
    "        <li>if both the partitions are from Climate data we are directly ingesting to the database without performing any transformation</li>\n",
    "    </ul>\n",
    "    <li>If the partitions do not match with geohashes, we directly ingest the data to the database. we do this to ensure that we do not lose data irrespective of whether the data is useful, relevant or not. The streamed and ingested data maybe of significance and prove to be a historic data for processing sometime later.</li>   \n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# importing os package to set the environment for the pyspark streaming application\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.3.0 pyspark-shell'\n",
    "\n",
    "# importing all other relevant libraries\n",
    "import ast\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "from pymongo import MongoClient\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "import pygeohash as pgh\n",
    "import json\n",
    "import pandas as pd\n",
    "import string\n",
    "from random import sample, choice\n",
    "import random\n",
    "\n",
    "\n",
    "# function to ingest the streamed data to the database to respective collections\n",
    "def sendDataToDB(iter):\n",
    "    \n",
    "    #establishing a connection to the client\n",
    "    client = MongoClient()\n",
    "    \n",
    "    #creating a database/referencing an existing database if it exists\n",
    "    db = client.assignment2\n",
    "    \n",
    "    # creating or referencing an existing colletion for climate data\n",
    "    climate = db.climate\n",
    "    \n",
    "    # creating or referencing an existing collection for hotspot data\n",
    "    fire = db.fire\n",
    "    \n",
    "    # list to hold the collections of a particular partitioned stream\n",
    "    lst=[]\n",
    "    \n",
    "    # counter to keep track of the number of chunks in the partition\n",
    "    count = 0\n",
    "    \n",
    "    # A loop that will go through each of the record of the partition and perform operations based on the conditions satisfied\n",
    "    for record in iter:\n",
    "        \n",
    "        # creating a json object of the read data\n",
    "        x =json.loads(str(record[1]).replace(\"'\",'\"'))\n",
    "        \n",
    "        # creating a new dictionary for each of the json objects read to append additional information\n",
    "        ins = {}\n",
    "        \n",
    "        # incrementing the count for each of the partition\n",
    "        count += 1\n",
    "        \n",
    "        # replicating the data items in the dictionary to the new dictionary\n",
    "        for k,v in x.items():\n",
    "            ins[k] = x[k]\n",
    "        \n",
    "        # appending geohash to the new dictionary with all the values of the existing dictionary with geohash as the key\n",
    "        ins[\"geohash\"] = pgh.encode(float(x[\"latitude\"]),float(x[\"longitude\"]), precision = 5)\n",
    "        \n",
    "        # appending the updated dictionary to a list for further processing\n",
    "        lst.append(ins)        \n",
    "    \n",
    "    # check if the number of partitions in the streamed partition is more than 1\n",
    "    if count > 1:\n",
    "        \n",
    "        # perform a serial comparison with every element in the list with every other element and check conditions\n",
    "        for i in range(len(lst)):\n",
    "            for j in range(i+1,len(lst)):\n",
    "                \n",
    "                # checking if the first 3 prefixes are equal\n",
    "                if lst[i][\"geohash\"][:3] == lst[j][\"geohash\"][:3]:\n",
    "                    \n",
    "                    # checking if the participating partitions are from Producer 2 and Producer 3, and perfrom the average operation and ingest to the database\n",
    "                    if lst[i][\"sender_id\"] in [\"fire_AQUA_producer_2\",\"fire_TERRA_producer_3\"] and lst[j][\"sender_id\"] in [\"fire_AQUA_producer_2\",\"fire_TERRA_producer_3\"]:\n",
    "                        lst[i][\"confidence\"] = (lst[i][\"confidence\"]+lst[j][\"confidence\"])/2\n",
    "                        lst[i][\"surface_temperature_celcius\"] = (lst[i][\"surface_temperature_celcius\"]+lst[j][\"surface_temperature_celcius\"])/2\n",
    "                        lst[i][\"match\"] = lst[j][\"_id\"]    \n",
    "                        fire.insert(lst[i])\n",
    "                    \n",
    "                    # checking if both the partition are from climate, we ingest both of them to the climate collection\n",
    "                    elif lst[i][\"sender_id\"] == \"climate_producer_1\" and lst[j][\"sender_id\"] == \"climate_producer_1\":\n",
    "\n",
    "                        climate.insert(lst[i])\n",
    "                        climate.insert(lst[j])\n",
    "\n",
    "                    else:\n",
    "                        # if both the above conditions are not met, then one must be from climate and the other from\n",
    "                        # hotspot, we create a new key \"match\" with ObjectId of the matched hotspot data and eventually \n",
    "                        # insert into database.\n",
    "                        if lst[i][\"sender_id\"] == \"climate_producer_1\":\n",
    "                            lst[i][\"match\"] = lst[j][\"_id\"]\n",
    "\n",
    "                            climate.insert(lst[i])\n",
    "                            fire.insert(lst[j])\n",
    "\n",
    "                        else:\n",
    "                            lst[j][\"match\"] = lst[i][\"_id\"]\n",
    "\n",
    "                            climate.insert(lst[j])\n",
    "                            fire.insert(lst[i])\n",
    "                \n",
    "                # if the geohashes do not match, identify the producer of the datachunk and insert it into database. Once added\n",
    "                # we break the inner loop to prevent comparison of the same datachunk and insertion. this will prevent duplication \n",
    "                # of data in the database\n",
    "                else:\n",
    "                    if lst[i][\"sender_id\"] == \"climate_producer_1\":\n",
    "                        climate.insert(lst[i])\n",
    "                        break\n",
    "                    else:\n",
    "                        fire.insert(lst[i])\n",
    "                        break\n",
    "    \n",
    "    # if the size of the data partition is 1 and is from Producer 1, we directly insert the data to the database \n",
    "    else:\n",
    "        if lst[i][\"sender_id\"] == \"climate_producer_1\":\n",
    "            climate.insert(lst[i])\n",
    "            \n",
    "    # close the connection to the database, after the database operation are completed\n",
    "    client.close()\n",
    "        \n",
    "\n",
    "        \n",
    "# time interval for the streaming application to run\n",
    "n_secs = 10\n",
    "\n",
    "# topic for the streaming spark application to connect.\n",
    "topic = \"test\"\n",
    "\n",
    "# setting a local streaming application with 2 threads.\n",
    "conf = SparkConf().setAppName(\"KafkaStreamProcessor\").setMaster(\"local[2]\")\n",
    "\n",
    "# creating or getting a spark streaming context\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "if sc is None:\n",
    "    sc = SparkContext(conf=conf)\n",
    "sc.setLogLevel(\"WARN\")\n",
    "\n",
    "# streaming context to run using the created spark streaming context with the specified number of seconds\n",
    "ssc = StreamingContext(sc, n_secs)\n",
    "\n",
    "# creating a direct stream with the specified topic and the Kafka server instance\n",
    "kafkaStream = KafkaUtils.createDirectStream(ssc, [topic], {\n",
    "                        'bootstrap.servers':'127.0.0.1:9092', \n",
    "                        'group.id':'week11-group', \n",
    "                        'fetch.message.max.bytes':'15728640',\n",
    "                        'auto.offset.reset':'largest'})\n",
    "                        # Group ID is completely arbitrary\n",
    "\n",
    "# Printing the contents of the read stream\n",
    "kafkaStream.pprint()\n",
    "\n",
    "# for each parritioned RDD calling the sendDataToDB\n",
    "lines = kafkaStream.foreachRDD(lambda rdd: rdd.foreachPartition(sendDataToDB))\n",
    "\n",
    "# starting the streaming context and await till termination\n",
    "ssc.start()\n",
    "\n",
    "# Run stream for 10 minutes just in case no detection of producer\n",
    "time.sleep(600) \n",
    "ssc.awaitTermination()\n",
    "ssc.stop(stopSparkContext=True,stopGraceFully=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
