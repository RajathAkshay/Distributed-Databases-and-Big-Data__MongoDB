{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EVENT PRODUCER 1- Streaming of Climate Data <hr />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Part C of the assignment, the objective is to simulate a real-time streaming of the data. The data will be streamed by using Kafka Producers and sent/streamed across a channel(topic) which would be processed by a Spark Streaming client and inject the mongodb database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Event Producer 1 </b> - The event producer 1 will be responsible for streaming climate data from climate_streaming.csv file and inject the topic(channel) <b>'test'</b> in a randomized fashion to masquerade a real application. Each chunk of the data that is streamed is appended with additional information in terms of sender_id and created_time. Furthermore, the data will be streamed every 5 seconds to model a real-world application with plausible latency.\n",
    "\n",
    "We will be activating all the Producers at the same time. We will be following a single channel approach throughout the implementation of the streaming application which means, all the producers will be streaming data to a single topic and the spark streaming client listens to the exact same topic and and then process data chunks in partitions where each parition consists of data chunks from all the data producers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imporing libraries\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "from json import dumps\n",
    "from kafka import KafkaProducer\n",
    "import random\n",
    "import datetime as dt\n",
    "\n",
    "# function to publish the data to the topic with an established connection. the data recieved will be sent in a key \n",
    "# value format. key being an indicator and the value being the stringified form of a dictionary(json) which will be easier\n",
    "# to be consumed and processed and insertable format for mongoDB.\n",
    "def publish_message(producer_instance, topic_name, key, value):\n",
    "    try:\n",
    "        # encoding the key and value in utf-8 format.\n",
    "        key_bytes = bytes(key, encoding='utf-8')\n",
    "        value_bytes = bytes(value, encoding='utf-8')\n",
    "        \n",
    "        # sending the data to the specified topic with key and value as encoded strings from the passed producer instance.\n",
    "        producer_instance.send(topic_name, key=key_bytes, value=value_bytes)\n",
    "        \n",
    "        # wait for the all the messages in the queue to be delivered to the topic until the message queue gets empty or the\n",
    "        # producer runs out of time\n",
    "        producer_instance.flush()\n",
    "        print('Message published successfully. Data: ' + str(value))\n",
    "    except Exception as ex:\n",
    "        print('Exception in publishing message.')\n",
    "        print(str(ex))\n",
    "   \n",
    "\n",
    "        # Function to create a connection to the Kafka instance that accepts connection on port 9092. The function returns an instance of the \n",
    "        # connected object. \n",
    "def connect_kafka_producer():\n",
    "    _producer = None\n",
    "    try:\n",
    "        # establishing a connection to the kafka instance and assign the instance to a variable. \n",
    "        _producer = KafkaProducer(bootstrap_servers=['localhost:9092'],\n",
    "                                  api_version=(0, 10))\n",
    "    except Exception as ex:\n",
    "        print('Exception while connecting Kafka.')\n",
    "        print(str(ex))\n",
    "    finally:\n",
    "        return _producer\n",
    "\n",
    "# function to the read data as a dataframe and convert the dataframe to a dictionary\n",
    "def read_data():\n",
    "    climate_stream = pd.read_csv(\"climate_streaming.csv\")\n",
    "    climate_dict = climate_stream.to_dict(orient = \"records\")\n",
    "    return climate_dict\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "   \n",
    "    # initiating a topic 'test' for the prodcuer to connect to.\n",
    "    topic = 'test'\n",
    "    \n",
    "    print('Publishing records..')\n",
    "    \n",
    "    # read the data as a dictionary\n",
    "    x = read_data()\n",
    "    \n",
    "    # get a connection and get a producer instance\n",
    "    producer = connect_kafka_producer()\n",
    "    \n",
    "    # send each dictioanry(json) as a data chunk. all the data chunks will be sent. \n",
    "    for i in range(0,len(x)):\n",
    "        \n",
    "        # generate a random number from 0 to length of the entire dataset. The randomly generated random number will be used as the \n",
    "        # index to extract the dictionary at the attained index would be fetched. However this also shows that there can be duplicates data chunks that could be spent across.\n",
    "        index = random.randrange(0,len(x))\n",
    "        \n",
    "        # appending the extracted index with a sender id.\n",
    "        x[index][\"sender_id\"] = \"climate_producer_1\"\n",
    "        \n",
    "        # appending the created_time to the fetched data.\n",
    "        x[index][\"created_time\"] = dt.datetime.now().strftime(\"%X\")\n",
    "        \n",
    "        # send across the data to the specified topic as a stringified dictionary.\n",
    "        publish_message(producer, topic, 'parsed', str(x[index]))\n",
    "        \n",
    "        # the producers waits for 5 seconds until it sends the next chunk of data.\n",
    "        sleep(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
