{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EVENT PRODUCER 3- Streaming of Hotspot Terra Data<hr />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Event Producer 3 </b> - The event producer 3 will be responsible for streaming climate data from hotspot_TERRA_streaming.csv file and inject the topic(channel) <b>'test'</b> in a randomized fashion to masquerade a real application. Each chunk of the data that is streamed is appended with additional information in terms of sender_id and created_time. Furthermore, the data will be streamed every 3 seconds to model a real-world application with plausible latency. \n",
    "\n",
    "This producer produces data every 3 seconds. We have decided to go with 3 seconds as the streaming client application needs to have more fire data to make a possible relation among the climate and hotspot data more possible. Although this digresses from the specification, we have a plausible reason for this digression as we are trying to make it more proabable for any two data chunks from different sources to be closely related. This is essentially important as we are streaming data in a randomized manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imporing libraries\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "from json import dumps\n",
    "from kafka import KafkaProducer\n",
    "import random\n",
    "import datetime as dt\n",
    "\n",
    "\n",
    "# function to publish the data to the topic with an established connection. the data recieved will be sent in a key \n",
    "# value format. key being an indicator and the value being the stringified form of a dictionary(json) which will be easier\n",
    "# to be consumed and processed and insertable format for mongoDB.\n",
    "def publish_message(producer_instance, topic_name, key, value):\n",
    "    try:\n",
    "        # encoding the key and value in utf-8 format.\n",
    "        key_bytes = bytes(key, encoding='utf-8')\n",
    "        value_bytes = bytes(value, encoding='utf-8')\n",
    "        \n",
    "        # sending the data to the specified topic with key and value as encoded strings from the passed producer instance.\n",
    "        producer_instance.send(topic_name, key=key_bytes, value=value_bytes)\n",
    "        \n",
    "        # wait for the all the messages in the queue to be delivered to the topic until the message queue gets empty or the\n",
    "        # producer runs out of time\n",
    "        producer_instance.flush()\n",
    "        print('Message published successfully. Data: ' + str(value))\n",
    "    except Exception as ex:\n",
    "        print('Exception in publishing message.')\n",
    "        print(str(ex))\n",
    "\n",
    "# Function to create a connection to the Kafka instance that accepts connection on port 9092. The function returns an instance of the \n",
    "# connected object. \n",
    "def connect_kafka_producer():\n",
    "    _producer = None\n",
    "    try:\n",
    "        # establishing a connection to the kafka instance and assign the instance to a variable. \n",
    "        _producer = KafkaProducer(bootstrap_servers=['localhost:9092'],\n",
    "                                  api_version=(0, 10))\n",
    "    except Exception as ex:\n",
    "        print('Exception while connecting Kafka.')\n",
    "        print(str(ex))\n",
    "    finally:\n",
    "        return _producer\n",
    "\n",
    "\n",
    "# function to the read data as a dataframe and convert the dataframe to a dictionary\n",
    "def read_data():\n",
    "    hotspot_TERRA_stream = pd.read_csv(\"hotspot_TERRA_streaming.csv\")\n",
    "    TERRA_dict = hotspot_TERRA_stream.to_dict(orient = \"records\")\n",
    "    return TERRA_dict\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "   \n",
    "    # initiating a topic 'test' for the prodcuer to connect to.\n",
    "    topic = 'test'\n",
    "    \n",
    "    \n",
    "    print('Publishing records..')\n",
    "    \n",
    "    # read the data as a dictionary\n",
    "    x = read_data()\n",
    "    \n",
    "    # get a connection and get a producer instance\n",
    "    producer = connect_kafka_producer()\n",
    "    \n",
    "    # send each dictioanry(json) as a data chunk. all the data chunks will be sent. \n",
    "    for i in range(0,len(x)):\n",
    "        \n",
    "        # generate a random number from 0 to length of the entire dataset. The randomly generated random number will be used as the \n",
    "        # index to extract the dictionary at the attained index would be fetched. However this also shows that there can be duplicates data chunks that could be spent across.\n",
    "        index = random.randrange(0,len(x))\n",
    "        \n",
    "        # appending the extracted index with a sender id.\n",
    "        x[index][\"sender_id\"] = \"fire_TERRA_producer_3\"\n",
    "        \n",
    "        # appending the created_time to the fetched data.\n",
    "        x[index][\"created_time\"] = dt.datetime.now().strftime(\"%X\")\n",
    "        \n",
    "        # send across the data to the specified topic as a stringified dictionary.\n",
    "        publish_message(producer, topic, 'parsed', str(x[index]))\n",
    "        \n",
    "        # the producers waits for 3 seconds until it sends the next chunk of data.\n",
    "        sleep(3)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
